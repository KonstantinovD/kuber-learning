
- 1.0) ///// K8S INFO /////
--- Зачем нужен кубер?
1. Env variables, хранение secrets(token/password)
2. Автоскейлинг
3. Stateful - приложения сохраняют свои данные (database, cache, etc)
4. healthchecks
5. настройка ресурсов

--- Инструменты разработчика
1. Minikube - в одну команду поднять маленький кластер
2. Ksync
3. Telepresence
4. Skaffold

--- Отладка
1. cubectl-debug
2. cubectl logs/describe

--- Темплейтирование
1. Helm
2. Kustomise

--- Можно быстро поднять окружение под разработку

- 1.1) /// Схема кубера /
CLI -> API ---> Master (набор управляющих компонентов кубера)
        |                         |
   K8S Storage                Workers (на них ставятся поды)
                              (worker-1, worker-2, etc)
							  
kind get kubeconfig-path --name="kind-control-plane"


2.0) KIND

1) создать кластер 
kind create cluster --name=example2

2) get base info
C:\Users\User>kubectl cluster-info --context kind-kind
>Kubernetes control plane is running at https://127.0.0.1:59537
>CoreDNS is running at https://127.0.0.1:59537/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


3) kind - get config
(находится в C:\Users\User\.kube\config)

4) 
kubectl get namespaces

5)
kubectl get pods --all-namespaces=true
>NAMESPACE            NAME                                         READY   STATUS    RESTARTS         AGE
>kube-system          coredns-64897985d-92wqx                      1/1     Running   1 (6m5s ago)     25h
>kube-system          coredns-64897985d-dz4n5                      1/1     Running   1 (6m6s ago)     25h
>kube-system          etcd-kind-control-plane                      1/1     Running   1 (6m9s ago)     25h
>kube-system          kindnet-x8s9c                                1/1     Running   2 (6m7s ago)     25h
>kube-system          kube-apiserver-kind-control-plane            1/1     Running   3 (6m6s ago)     25h
>kube-system          kube-controller-manager-kind-control-plane   1/1     Running   22 (6m7s ago)    25h
>kube-system          kube-proxy-vv9r4                             1/1     Running   1 (6m3s ago)     25h
>kube-system          kube-scheduler-kind-control-plane            1/1     Running   20 (6m7s ago)    25h
>local-path-storage   local-path-provisioner-5ddd94ff66-sx7kk      1/1     Running   16 (3m37s ago)   25h


Pod - минимальная единица (абстракция) кубера - одно приложение в кластере
Внутри пода есть контейнеры, в 99% случаев 1 под - 1 контейнер
Но! Внутри одного пода всегда 2 контейнера - сначала контейнер POD - под сетевого контейнера (создает сеть)
С контейнерами НАПРЯМУЮ кубер не работает

Пример сильной связи (два контейнера в одном поде): prometheus + config-reloader + экспортер
Прометей не умеет отслеживать изменения в своем конфиге сам, поэтому
релоадер отслеживает их за него и дергает веб апи прометея 
по локалхосту (они в одном поде) - так быстрее
То же самое и про blackbox-exporter
Если у вас два прометея - у вас два релоадера, два экспортера

Service-mesh - добавляет еще один контейнер с проксей

6.0) Создание пода
kubectl create -f school-dev-k8s-main/practice/3.application-abstractions/1.pod/pod.yaml
>pod/my-pod created
kubectl get pod
>NAME     READY   STATUS              RESTARTS   AGE
>my-pod   0/1     ContainerCreating   0          11s

меняем:
metadata:
  name: my-pod-1
  
6.1) запускаем новый под
>kubectl create -f school-dev-k8s-main/practice/3.application-abstractions/1.pod/pod.yaml
pod/my-pod-1 created
>kubectl get pod
NAME       READY   STATUS              RESTARTS   AGE
my-pod     1/1     Running             0          8m41s
my-pod-1   0/1     ContainerCreating   0          4s

>kubectl get pod
NAME       READY   STATUS    RESTARTS   AGE
my-pod     1/1     Running   0          8m45s
my-pod-1   1/1     Running   0          8s

7) описание пода:
>kubectl describe pod my-pod
Name:         my-pod
Namespace:    default
Priority:     0
Node:         kind-control-plane/172.19.0.2
Start Time:   Wed, 13 Apr 2022 00:23:03 +0300
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.244.0.5
IPs:
  IP:  10.244.0.5
Containers:
  nginx:
    Container ID:   containerd://4f2d701153cd27e895346f5da7d739ff2390b128fa1786bfb0d6f7a869c52608
    Image:          quay.io/testing-farm/nginx:1.12
    Image ID:       quay.io/testing-farm/nginx@sha256:09e210fe1e7f54647344d278a8d0dee8a4f59f275b72280e8b5a7c18c560057f
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 13 Apr 2022 00:31:10 +0300
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l6mr6 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-l6mr6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m25s  default-scheduler  Successfully assigned default/my-pod to kind-control-plane
  Normal  Pulling    9m24s  kubelet            Pulling image "quay.io/testing-farm/nginx:1.12"
  Normal  Pulled     80s    kubelet            Successfully pulled image "quay.io/testing-farm/nginx:1.12" in 8m4.2766719s
  Normal  Created    78s    kubelet            Created container nginx
  Normal  Started    78s    kubelet            Started container nginx

8.0) Удаление пода
>kubectl delete pod my-pod
pod "my-pod" deleted

8.1) ЛИБО удаляем через ямл
>kkubectl delete -f school-dev-k8s-main/practice/3.application-abstractions/1.pod/pod.yaml
pod "my-pod-1" deleted

